{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in all dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import date\n",
    "from pprint import pprint\n",
    "import sqlalchemy\n",
    "#from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy import Column, Integer, String, Float\n",
    "from config_poly import api_key\n",
    "import sqlite3\n",
    "#from splinter import Browser\n",
    "#from bs4 import BeautifulSoup\n",
    "#from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish link to Russell 100 index wiki page\n",
    "russell_url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n",
    "\n",
    "# Creaters browser window for scraping\n",
    "#executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "#browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visits url, creates soup object\n",
    "#browser.visit(russell_url)\n",
    "#soup = BeautifulSoup(browser.html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       2U\n",
       "1                       3M\n",
       "2      Abbott Laboratories\n",
       "3              AbbVie Inc.\n",
       "4                  Abiomed\n",
       "              ...         \n",
       "952                 Xilinx\n",
       "953          XPO Logistics\n",
       "954             Xylem Inc.\n",
       "955     Yum China Holdings\n",
       "956            Yum! Brands\n",
       "Name: Company, Length: 957, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = pd.read_html(russell_url)\n",
    "company_table = thing[2]\n",
    "companies = company_table[\"Company\"]\n",
    "myCompanies = pd.unique(companies)\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = company_table[\"Ticker\"]\n",
    "myTickers = tickers.drop_duplicates()\n",
    "len(myTickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 100\n",
    "# newTickers = [tickers[i:i + n] for i in range(0, len(tickers), n)]\n",
    "# Should be 10 lists\n",
    "# newTickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On BKR, error trying to call\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to hold information from polygon.io\n",
    "polygon_info = []\n",
    "polygon_div = []\n",
    "\n",
    "startTime = time.time()\n",
    "#today = date.today()\n",
    "#todays_date = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Initializes counter to 0\n",
    "count = 0\n",
    "\n",
    "error_log = open(\"errors.log\",\"a\")\n",
    "    \n",
    "# Loops through one list of ticker symbols\n",
    "for ticker in tickers:\n",
    "\n",
    "    # Checks if count equals 4, used to prevent exceeding \n",
    "    # the API call limit of 5 calls/minute\n",
    "    if count == 4:\n",
    "\n",
    "        # Tells program to wait 1.5 minutes before running again\n",
    "        time.sleep(60)\n",
    "\n",
    "        # Ticker symbols to be used for API Call\n",
    "        dataset_code = ticker\n",
    "\n",
    "        # URLs for API Calls, the first for open and closed prices, \n",
    "        # the second for divident information\n",
    "        url = f\"https://api.polygon.io/v1/open-close/{dataset_code}/2021-04-23?unadjusted=true&apiKey={api_key}\"\n",
    "        div_url = f\"https://api.polygon.io/v2/reference/dividends/{dataset_code}?&apiKey={api_key}\"\n",
    "\n",
    "        # Saving responses from both urls\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            div_response = requests.get(div_url).json()\n",
    "\n",
    "            # Adding responses to info and div lists, respectively\n",
    "            polygon_info.append(response)\n",
    "            polygon_div.append(div_response)\n",
    "\n",
    "        except:\n",
    "            error_log.write(ticker)\n",
    "            print(f\"On {ticker}, error trying to call\")\n",
    "\n",
    "        # Resets counter\n",
    "        count = 0\n",
    "\n",
    "    else:\n",
    "        # Ticker symbols to be used for API Call\n",
    "        dataset_code = ticker\n",
    "\n",
    "        # URLs for API Calls, the first for open and closed prices, \n",
    "        # the second for divident information\n",
    "        url = f\"https://api.polygon.io/v1/open-close/{dataset_code}/2021-04-23?unadjusted=true&apiKey={api_key}\"\n",
    "        div_url = f\"https://api.polygon.io/v2/reference/dividends/{dataset_code}?&apiKey={api_key}\"\n",
    "\n",
    "         # Saving responses from both urls\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            div_response = requests.get(div_url).json()\n",
    "\n",
    "            # Adding responses to info and div lists, respectively\n",
    "            polygon_info.append(response)\n",
    "            polygon_div.append(div_response)\n",
    "\n",
    "        except:\n",
    "            error_log.write(ticker)\n",
    "            print(f\"On {ticker}, error trying to call\")\n",
    "\n",
    "        # Increase counter by 2\n",
    "        count += 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496.2675666451454"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executionTime = (time.time() - startTime)\n",
    "\n",
    "realTime = executionTime/60\n",
    "realTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'error.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-caf19705de0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0merror2_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errors_2.log\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mread_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error.log\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'error.log'"
     ]
    }
   ],
   "source": [
    "error2_log = open(\"errors_2.log\",\"a\")\n",
    "\n",
    "read_error = open(\"errors.log\",\"r\")\n",
    "startTime = time.time()\n",
    "\n",
    "# Loops through one list of ticker symbols\n",
    "for ticker in read_error:\n",
    "\n",
    "    # Checks if count equals 4, used to prevent exceeding \n",
    "    # the API call limit of 5 calls/minute\n",
    "    if count == 4:\n",
    "\n",
    "        # Tells program to wait 1.5 minutes before running again\n",
    "        time.sleep(60)\n",
    "\n",
    "        # Ticker symbols to be used for API Call\n",
    "        dataset_code = ticker\n",
    "\n",
    "        # URLs for API Calls, the first for open and closed prices, \n",
    "        # the second for divident information\n",
    "        url = f\"https://api.polygon.io/v1/open-close/{dataset_code}/2021-04-23?unadjusted=true&apiKey={api_key}\"\n",
    "        div_url = f\"https://api.polygon.io/v2/reference/dividends/{dataset_code}?&apiKey={api_key}\"\n",
    "\n",
    "        # Saving responses from both urls\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            div_response = requests.get(div_url).json()\n",
    "\n",
    "            # Adding responses to info and div lists, respectively\n",
    "            polygon_info.append(response)\n",
    "            polygon_div.append(div_response)\n",
    "\n",
    "        except:\n",
    "            error_log.write(ticker)\n",
    "            print(f\"On {ticker}, error trying to call\")\n",
    "\n",
    "        # Resets counter\n",
    "        count = 0\n",
    "\n",
    "    else:\n",
    "        # Ticker symbols to be used for API Call\n",
    "        dataset_code = ticker\n",
    "\n",
    "        # URLs for API Calls, the first for open and closed prices, \n",
    "        # the second for divident information\n",
    "        url = f\"https://api.polygon.io/v1/open-close/{dataset_code}/2021-04-23?unadjusted=true&apiKey={api_key}\"\n",
    "        div_url = f\"https://api.polygon.io/v2/reference/dividends/{dataset_code}?&apiKey={api_key}\"\n",
    "\n",
    "         # Saving responses from both urls\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            div_response = requests.get(div_url).json()\n",
    "\n",
    "            # Adding responses to info and div lists, respectively\n",
    "            polygon_info.append(response)\n",
    "            polygon_div.append(div_response)\n",
    "\n",
    "        except:\n",
    "            error2_log.write(ticker)\n",
    "            print(f\"On {ticker}, error trying to call\")\n",
    "\n",
    "        # Increase counter by 2\n",
    "        count += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executionTime = (time.time() - startTime)\n",
    "\n",
    "realTime = executionTime/60\n",
    "realTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_table = pd.DataFrame({'Name':myCompanies,\"Symbol\":myTickers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_info_df = pd.DataFrame(polygon_info)\n",
    "del polygon_info_df['status']\n",
    "del polygon_info_df['afterHours']\n",
    "del polygon_info_df['preMarket']\n",
    "polygon_info_df = polygon_info_df[['symbol','from','open','high','low','close','volume']]\n",
    "polygon_info_df.columns = ['symbol','date','open','high','low','close','volume']\n",
    "\n",
    "polygon_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_div = polygon_div[1:]\n",
    "div_amt_list = []\n",
    "payDate = []\n",
    "ticker_list = []\n",
    "\n",
    "for div in all_div:\n",
    "    result = div['results']\n",
    "    \n",
    "    if result == []:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        ticker = div[\"results\"][0]['ticker']\n",
    "        div_amt = div[\"results\"][0]['amount']\n",
    "        pay_date = div[\"results\"][0]['paymentDate']\n",
    "    \n",
    "    ticker_list.append(ticker)\n",
    "    div_amt_list.append(div_amt)\n",
    "    payDate.append(pay_date)\n",
    "    \n",
    "polygon_div_df = pd.DataFrame({\"Symbol\":ticker_list,\"Amount ($)\":div_amt_list,\"Payment Date\":payDate})\n",
    "polygon_div_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_info_df.to_csv('openClose.csv')\n",
    "polygon_div_df.to_csv('dividend.csv')\n",
    "company_table.to_csv('companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_info_df = pd.read_csv('openClose.csv')\n",
    "polygon_div_df = pd.read_csv('dividend.csv')\n",
    "company_table = pd.read_csv('companies.csv')\n",
    "\n",
    "polygon_info_df.columns = ['index','symbol','date','opening','high','low','close','volume']\n",
    "polygon_info_df = polygon_info_df.drop(columns='index')\n",
    "polygon_div_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del polygon_div_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del company_table['Unnamed: 0']\n",
    "company_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sqlite database path\n",
    "database_path = \"russell_stock.sqlite\"\n",
    "\n",
    "# Create an engine that can talk to the database\n",
    "engine = create_engine(f\"sqlite:///{database_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use engine to connect to existing tables/db\n",
    "Base = declarative_base()\n",
    "#conn = sqlite3.connect('russell_stock.sqlite')\n",
    "#c = conn.cursor()\n",
    "\n",
    "# Use this to clear out the db\n",
    "# ----------------------------------\n",
    "Base.metadata.drop_all(bind=engine)\n",
    "\n",
    "# Create a \"Metadata\" Layer That Abstracts our SQL Database\n",
    "# ----------------------------------\n",
    "#Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our General_Info table\n",
    "class Open_Close(Base):\n",
    "    __tablename__ = 'openClose'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    symbol = Column(String, primary_key=True)\n",
    "    date = Column(String)\n",
    "    opening = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Integer)\n",
    "\n",
    "# Define our Dividend table\n",
    "class Dividend(Base):\n",
    "    __tablename__ = 'dividend'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    symbol = Column(String, primary_key=True)\n",
    "    amount = Column(Float)\n",
    "    payment_date = Column(String)\n",
    "\n",
    "# Define our Company Table\n",
    "class Company(Base):\n",
    "    __tablename__ = 'company'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    symbol = Column(String, primary_key=True)\n",
    "    name = Column(String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save references to each table (capital because they are considered classes) \n",
    "#Company = Database.classes.company\n",
    "#Dividend = Database.classes.dividend\n",
    "#Open_Close = Database.classes.open_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in polygon_info_df.index:\n",
    "#    print(polygon_info_df['from'][i])\n",
    "\n",
    "\n",
    "# insert dataframe into sqlite tables\n",
    "company_table.to_sql(name='company', con=engine, if_exists='replace', index=True)\n",
    "polygon_div_df.to_sql(name='dividend', con=engine, if_exists='replace', index=True)\n",
    "polygon_info_df.to_sql(name='openClose', con=engine, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in polygon_info_df.index:\n",
    "#     thisFrom = polygon_info_df[\"date\"][i]\n",
    "#     thisSymbol = polygon_info_df[\"symbol\"][i]\n",
    "#     thisOpen = polygon_info_df[\"open\"][i]\n",
    "#     thisHigh = polygon_info_df[\"high\"][i]\n",
    "#    thisLow = polygon_info_df[\"low\"][i]\n",
    "#    thisClose = polygon_info_df[\"close\"][i]\n",
    "#    thisVol = polygon_info_df[\"volume\"][i]\n",
    "    \n",
    "    #row = [thisFrom,thisSymbol,thisOpen,thisHigh,thisLow,thisClose,thisVol]\n",
    "    \n",
    "#    thisCompany = Open_Close(date=thisFrom,symbol=thisSymbol,opening=thisOpen,high=thisHigh,low=thisLow,close=thisClose,volume=thisVol)\n",
    "#    session = Session(bind=engine)\n",
    "    #conn.execute('Insert into open_close values(?,?,?,?,?,?,?)', row)\n",
    "    #engine.execute()\n",
    "#    Session.configure(bind=engine)\n",
    "#    session.add(thisCompany)\n",
    "    #session.commit()\n",
    "    \n",
    "#session.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.close()\n",
    "#session.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.close()\n",
    "\n",
    "#conn.close()\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"sqlite:///{database_path}\")\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "# use engine to connect to existing tables/db\n",
    "base = automap_base()\n",
    "base.prepare(engine, reflect=True)\n",
    "\n",
    "# View all of the classes/tables that automap found\n",
    "thing = base.classes.keys()\n",
    "\n",
    "print(thing)\n",
    "\n",
    "# Save references to each table (capital because they are considered classes) \n",
    "#Company = Database.classes.company\n",
    "#Dividend = Database.classes.dividend\n",
    "inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
